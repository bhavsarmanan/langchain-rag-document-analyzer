Directory structure:
└── bhavsarmanan-langchain-rag-document-analyzer/
    ├── README.md
    ├── app.py
    ├── bhavsarmanan-langchain-rag-usecases.txt
    ├── create_template.py
    ├── design_page.py
    ├── ingest_main.py
    ├── invoke_main.py
    ├── load_documents.py
    └── load_models.py

================================================
FILE: README.md
================================================
# langchain-rag-usecases
This repo has uses cases for various rag implementation.



================================================
FILE: app.py
================================================
import streamlit as st
from load_models import load_models
from load_documents import text_loader
from langchain_text_splitters import CharacterTextSplitter
from langchain_ollama import OllamaEmbeddings
from langchain_community.vectorstores import FAISS

from langchain_ollama import ChatOllama


from langchain import hub
from langchain.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough
#from langchain_community.hub import pull as hub_pull

#defined functions
from ingest_main import ingest_in_faiss
from create_template import custom_template
from design_page import draw_sidebar
from invoke_main import invoke_llm


def format_docs(documents):
    return "\n\n".join(documents.page_content for documents in documents)



st.title("Document Analyzer")
output=""

models = load_models()
loader = draw_sidebar()

if loader is not None:
    document = loader.load()


    print("Splitting text...")
    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=30,separator="\n")
    texts = text_splitter.split_documents(document)
    #st.write(texts)

    print("Embedding text...")
    embeddings = OllamaEmbeddings(model="llama3.2")
  

    
    # Get embeddings for each chunk of text
    #embedded_texts = [embeddings.embed_query(text.page_content) for text in texts]

    vectorstore = ingest_in_faiss(texts, embeddings)
    retrival_qa_chat_prompt = hub.pull("langchain-ai/retrieval-qa-chat")

    #custom_rag_prompt = custom_template()
    #docsearch = FAISS(embedding_function=embeddings,index_name="rag_index",docstore=InMemoryDocstore({}))

    #custom_rag_prompt = PromptTemplate.from_template(custom_template)
    template = """ answer the quesition with just 1 or 2 sentences max.

            Also your name is Siri

    {context}

    question: {question}    

    answer:"""

    custom_rag_prompt = PromptTemplate.from_template(template)

#draw_sidebar()

    selected_model = st.radio(":blue[Select the model]", options=models,horizontal=True )
    #st.write(models)
    llm = ChatOllama(model=selected_model)
    rag_chain = (
                    {"context": vectorstore.as_retriever() | format_docs,"question":RunnablePassthrough()}
                    | custom_rag_prompt
                    | llm 
                )
    # Initialize chat history
    if "messages" not in st.session_state:
        st.session_state.messages = []


    # Display chat messages from history on app rerun
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # React to user input
    if prompt := st.chat_input("Response powered by " + selected_model ):
        # Display user message in chat message container
        st.chat_message("user").markdown(prompt)
        # Add user message to chat history
        st.session_state.messages.append({"role": "user", "content": prompt})

        res = rag_chain.invoke(prompt)
        #res = invoke_llm(llm,vectorstore,prompt,retrival_qa_chat_prompt)
        #st.write(res)
                

        
        # Display assistant response in chat message container
        with st.chat_message("ai"):
            #output = res['answer']
            output = res
            st.markdown(output)
            
        st.session_state.messages.append({"role": "assistant", "content": output})

    # Add assistant response to chat history



================================================
FILE: bhavsarmanan-langchain-rag-usecases.txt
================================================
Directory structure:
└── bhavsarmanan-langchain-rag-usecases/
    ├── README.md
    ├── app.py
    ├── create_template.py
    ├── design_page.py
    ├── ingest_main.py
    ├── invoke_main.py
    ├── load_documents.py
    └── load_models.py

================================================
FILE: README.md
================================================
# langchain-rag-usecases
This repo has uses cases for various rag implementation.



================================================
FILE: app.py
================================================
import streamlit as st
from load_models import load_models
from load_documents import text_loader
from langchain_text_splitters import CharacterTextSplitter
from langchain_ollama import OllamaEmbeddings
from langchain_community.vectorstores import FAISS

from langchain_ollama import ChatOllama


from langchain import hub
from langchain.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough
#from langchain_community.hub import pull as hub_pull

#defined functions
from ingest_main import ingest_in_faiss
from create_template import custom_template
from design_page import draw_sidebar
from invoke_main import invoke_llm


def format_docs(documents):
    return "\n\n".join(documents.page_content for documents in documents)



st.title("Document Analyzer")
output=""

models = load_models()
loader = draw_sidebar()

if loader is not None:
    document = loader.load()


    print("Splitting text...")
    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=30,separator="\n")
    texts = text_splitter.split_documents(document)
    #st.write(texts)

    print("Embedding text...")
    embeddings = OllamaEmbeddings(model="llama3.2")
  

    
    # Get embeddings for each chunk of text
    #embedded_texts = [embeddings.embed_query(text.page_content) for text in texts]

    vectorstore = ingest_in_faiss(texts, embeddings)
    retrival_qa_chat_prompt = hub.pull("langchain-ai/retrieval-qa-chat")

    #custom_rag_prompt = custom_template()
    #docsearch = FAISS(embedding_function=embeddings,index_name="rag_index",docstore=InMemoryDocstore({}))

    #custom_rag_prompt = PromptTemplate.from_template(custom_template)
    template = """ answer the quesition with just 1 or 2 sentences max.

            Also your name is Siri

    {context}

    question: {question}    

    answer:"""

    custom_rag_prompt = PromptTemplate.from_template(template)

#draw_sidebar()

    selected_model = st.radio(":blue[Select the model]", options=models,horizontal=True )
    #st.write(models)
    llm = ChatOllama(model=selected_model)
    rag_chain = (
                    {"context": vectorstore.as_retriever() | format_docs,"question":RunnablePassthrough()}
                    | custom_rag_prompt
                    | llm 
                )
    # Initialize chat history
    if "messages" not in st.session_state:
        st.session_state.messages = []


    # Display chat messages from history on app rerun
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # React to user input
    if prompt := st.chat_input("Response powered by " + selected_model ):
        # Display user message in chat message container
        st.chat_message("user").markdown(prompt)
        # Add user message to chat history
        st.session_state.messages.append({"role": "user", "content": prompt})

        res = rag_chain.invoke(prompt)
        #res = invoke_llm(llm,vectorstore,prompt,retrival_qa_chat_prompt)
        #st.write(res)
                

        
        # Display assistant response in chat message container
        with st.chat_message("ai"):
            #output = res['answer']
            output = res
            st.markdown(output)
            
        st.session_state.messages.append({"role": "assistant", "content": output})

    # Add assistant response to chat history



================================================
FILE: create_template.py
================================================
from langchain.prompts import PromptTemplate
def custom_template():
    template = """ answer the quesition with just 1 or 2 sentences max.

    Also your name is Siri

    {context}

    question: {question}    

    answer:
    """
    return PromptTemplate.from_template(template)


================================================
FILE: design_page.py
================================================
import streamlit as st
from load_documents import select_loader

def draw_sidebar():
    st.sidebar.title("Upload document to be analyzed")
    selection = st.sidebar.selectbox("Documents that you can upload", ["PDF", "Text File"])

    if st.sidebar.button("Clear chat history"):
        st.session_state.messages = []

    if selection == "PDF":
        file = st.sidebar.file_uploader("Upload a file", type=["pdf"])
    elif selection == "Text File":
        file = st.sidebar.file_uploader("Upload a file", type=["txt"])

    if file is not None:
        loader = select_loader(file, selection)
        return loader
    else:
        st.sidebar.error("Please upload a file")



    


================================================
FILE: ingest_main.py
================================================
from langchain_community.vectorstores import FAISS
from langchain_community.docstore import InMemoryDocstore
from uuid import uuid4
import faiss


def ingest_in_faiss(texts, embeddings):

    #print(texts)

    index = faiss.IndexFlatL2(len(embeddings.embed_query("Hello World")))
    vector_store = FAISS(
    embedding_function=embeddings,
    index=index,
    docstore=InMemoryDocstore(),
    index_to_docstore_id={},
    )

    uuids = [str(uuid4()) for _ in range(len(texts))]

    #vector_store.add_documents(documents=texts, ids=uuids)
    vector_store.from_documents(documents=texts,embedding=embeddings)
    
    vectorstore = FAISS.from_documents(documents=texts,embedding=embeddings)
    #vectorstore = FAISS.add_documents(documents=texts,ids=uuids,embedding=embeddings)
    return vectorstore

def ingest_in_pgvector():
    pass





================================================
FILE: invoke_main.py
================================================
from langchain import hub
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains.retrieval import create_retrieval_chain

def invoke_llm(llm,vectorstore,prompt,retrival_qa_chat_prompt):
    stuff_document_chain = create_stuff_documents_chain(llm,retrival_qa_chat_prompt)
    qa = create_retrieval_chain(retriever=vectorstore.as_retriever(),combine_docs_chain=stuff_document_chain)
    response = qa.invoke(input={'input':prompt})
    return response




================================================
FILE: load_documents.py
================================================
from langchain_community.document_loaders import TextLoader
from langchain_community.document_loaders import PyPDFLoader
import streamlit as st
import tempfile
import os

def text_loader(file) -> TextLoader:
    # Create a temporary file
    with tempfile.NamedTemporaryFile(delete=False, suffix='.txt') as tmp_file:
        tmp_file.write(file.getvalue())
        tmp_path = tmp_file.name
    return TextLoader(tmp_path)

def pdf_loader(file) -> PyPDFLoader:
    # Create a temporary file
    with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:
        tmp_file.write(file.getvalue())
        tmp_path = tmp_file.name
    return PyPDFLoader(tmp_path)

def select_loader(file, file_type):
    if file_type == "PDF":
        loader = pdf_loader(file)
    elif file_type == "Text File":
        loader = text_loader(file)
    return loader


================================================
FILE: load_models.py
================================================
import subprocess

def load_models():


    model_list = subprocess.run(['ollama', 'list'], 
                                capture_output=True, 
                                text=True)
        
    lines = model_list.stdout.strip().split('\n')

    models = []
    for line in lines[1:]:  # Skip the header line
        if line.strip():  # Check if line is not empty
            model_name = line.split()[0]
            if  model_name.split(':')[0] != 'qwen3' and model_name.split(':')[0] != 'snowflake-arctic-embed2' and model_name.split(':')[0] != 'mxbai-embed-large': # First column is model name
                models.append(model_name.split(':')[0])
            if model_name.split(':')[0] == 'qwen3':
                models.append(model_name)



    return models




================================================
FILE: create_template.py
================================================
from langchain.prompts import PromptTemplate
def custom_template():
    template = """ answer the quesition with just 1 or 2 sentences max.

    Also your name is Siri

    {context}

    question: {question}    

    answer:
    """
    return PromptTemplate.from_template(template)


================================================
FILE: design_page.py
================================================
import streamlit as st
from load_documents import select_loader

def draw_sidebar():
    st.sidebar.title("Upload document to be analyzed")
    selection = st.sidebar.selectbox("Documents that you can upload", ["PDF", "Text File"])

    if st.sidebar.button("Clear chat history"):
        st.session_state.messages = []

    if selection == "PDF":
        file = st.sidebar.file_uploader("Upload a file", type=["pdf"])
    elif selection == "Text File":
        file = st.sidebar.file_uploader("Upload a file", type=["txt"])

    if file is not None:
        loader = select_loader(file, selection)
        return loader
    else:
        st.sidebar.error("Please upload a file")



    


================================================
FILE: ingest_main.py
================================================
from langchain_community.vectorstores import FAISS
from langchain_community.docstore import InMemoryDocstore
from uuid import uuid4
import faiss


def ingest_in_faiss(texts, embeddings):

    #print(texts)

    index = faiss.IndexFlatL2(len(embeddings.embed_query("Hello World")))
    vector_store = FAISS(
    embedding_function=embeddings,
    index=index,
    docstore=InMemoryDocstore(),
    index_to_docstore_id={},
    )

    uuids = [str(uuid4()) for _ in range(len(texts))]

    #vector_store.add_documents(documents=texts, ids=uuids)
    vector_store.from_documents(documents=texts,embedding=embeddings)
    
    vectorstore = FAISS.from_documents(documents=texts,embedding=embeddings)
    #vectorstore = FAISS.add_documents(documents=texts,ids=uuids,embedding=embeddings)
    return vectorstore

def ingest_in_pgvector():
    pass





================================================
FILE: invoke_main.py
================================================
from langchain import hub
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains.retrieval import create_retrieval_chain

def invoke_llm(llm,vectorstore,prompt,retrival_qa_chat_prompt):
    stuff_document_chain = create_stuff_documents_chain(llm,retrival_qa_chat_prompt)
    qa = create_retrieval_chain(retriever=vectorstore.as_retriever(),combine_docs_chain=stuff_document_chain)
    response = qa.invoke(input={'input':prompt})
    return response




================================================
FILE: load_documents.py
================================================
from langchain_community.document_loaders import TextLoader
from langchain_community.document_loaders import PyPDFLoader
import streamlit as st
import tempfile
import os

def text_loader(file) -> TextLoader:
    # Create a temporary file
    with tempfile.NamedTemporaryFile(delete=False, suffix='.txt') as tmp_file:
        tmp_file.write(file.getvalue())
        tmp_path = tmp_file.name
    return TextLoader(tmp_path)

def pdf_loader(file) -> PyPDFLoader:
    # Create a temporary file
    with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:
        tmp_file.write(file.getvalue())
        tmp_path = tmp_file.name
    return PyPDFLoader(tmp_path)

def select_loader(file, file_type):
    if file_type == "PDF":
        loader = pdf_loader(file)
    elif file_type == "Text File":
        loader = text_loader(file)
    return loader


================================================
FILE: load_models.py
================================================
import subprocess

def load_models():


    model_list = subprocess.run(['ollama', 'list'], 
                                capture_output=True, 
                                text=True)
        
    lines = model_list.stdout.strip().split('\n')

    models = []
    for line in lines[1:]:  # Skip the header line
        if line.strip():  # Check if line is not empty
            model_name = line.split()[0]
            if  model_name.split(':')[0] != 'qwen3' and model_name.split(':')[0] != 'snowflake-arctic-embed2' and model_name.split(':')[0] != 'mxbai-embed-large': # First column is model name
                models.append(model_name.split(':')[0])
            if model_name.split(':')[0] == 'qwen3':
                models.append(model_name)



    return models

